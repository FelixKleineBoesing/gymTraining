{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossentropy method\n",
    "\n",
    "This notebook will teach you to solve reinforcement learning problems with crossentropy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_states=500, n_actions=6\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"n_states=%i, n_actions=%i\"%(n_states, n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create stochastic policy\n",
    "\n",
    "This time our policy should be a probability distribution.\n",
    "\n",
    "```policy[s,a] = P(take action a | in state s)```\n",
    "\n",
    "Since we still use integer state and action representations, you can use a 2-dimensional array to represent the policy.\n",
    "\n",
    "Please initialize policy __uniformly__, that is, probabililities of all actions should be equal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " ...\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "policy = np.ones((n_states, n_actions))/n_actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert type(policy) in (np.ndarray,np.matrix)\n",
    "assert np.allclose(policy,1./n_actions)\n",
    "assert np.allclose(np.sum(policy,axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play the game\n",
    "\n",
    "Just like before, but we also record all states and actions we took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_session(policy,t_max=10**4):\n",
    "    \"\"\"\n",
    "    Play game until end or for t_max ticks.\n",
    "    :param policy: an array of shape [n_states,n_actions] with action probabilities\n",
    "    :returns: list of states, list of actions and sum of rewards\n",
    "    \"\"\"\n",
    "    states,actions = [],[]\n",
    "    total_reward = 0.\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        a = np.random.choice(n_actions, 1, p=policy[s])[0]\n",
    "        \n",
    "        new_s, r, done, info = env.step(a)\n",
    "        \n",
    "        #Record state, action and add up reward to states,actions and total_reward accordingly. \n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "        \n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states, actions, total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s,a,r = generate_session(policy)\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) in [float,np.float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f751f2e4278>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncwa9JGXOoJekzBn0kpS5/w+zNmBzOySIHQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#let's see the initial reward distribution\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sample_rewards = [generate_session(policy,t_max=1000)[-1] for _ in range(200)]\n",
    "\n",
    "plt.hist(sample_rewards,bins=20);\n",
    "plt.vlines([np.percentile(sample_rewards, 50)], [0], [100], label=\"50'th percentile\", color='green')\n",
    "plt.vlines([np.percentile(sample_rewards, 90)], [0], [100], label=\"90'th percentile\", color='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossentropy method steps (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_elites(states_batch, actions_batch, rewards_batch, percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i][t]\n",
    "    \n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "    \n",
    "    Please return elite states and actions in their original order \n",
    "    [i.e. sorted by session number and timestep within session]\n",
    "    \n",
    "    If you're confused, see examples below. Please don't assume that states are integers (they'll get different later).\n",
    "    \"\"\"\n",
    "\n",
    "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "\n",
    "    elite_states = [state for index in range(len(states_batch)) if rewards_batch[index] >= reward_threshold for state in states_batch[index]]\n",
    "    elite_actions = [action for index in range(len(actions_batch)) if rewards_batch[index] >= reward_threshold for action in actions_batch[index]]\n",
    "\n",
    "    return elite_states, elite_actions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "[0, 2, 4, 3, 2, 0, 1, 3, 3]\n",
      "3.6\n",
      "[3, 2, 0, 1, 3, 3]\n",
      "4.8\n",
      "[3, 3]\n",
      "5.0\n",
      "[3, 3]\n",
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "states_batch = [\n",
    "    [1,2,3],   #game1\n",
    "    [4,2,0,2], #game2\n",
    "    [3,1]      #game3\n",
    "]\n",
    "\n",
    "actions_batch = [\n",
    "    [0,2,4],   #game1\n",
    "    [3,2,0,1], #game2\n",
    "    [3,3]      #game3\n",
    "]\n",
    "rewards_batch = [\n",
    "    3,         #game1\n",
    "    4,         #game2\n",
    "    5,         #game3\n",
    "]\n",
    "\n",
    "test_result_0 = select_elites(states_batch, actions_batch, rewards_batch, percentile=0)\n",
    "test_result_40 = select_elites(states_batch, actions_batch, rewards_batch, percentile=30)\n",
    "test_result_90 = select_elites(states_batch, actions_batch, rewards_batch, percentile=90)\n",
    "test_result_100 = select_elites(states_batch, actions_batch, rewards_batch, percentile=100)\n",
    "\n",
    "assert np.all(test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1])  \\\n",
    "   and np.all(test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]),\\\n",
    "        \"For percentile 0 you should return all states and actions in chronological order\"\n",
    "assert np.all(test_result_40[0] == [4, 2, 0, 2, 3, 1]) and \\\n",
    "        np.all(test_result_40[1] ==[3, 2, 0, 1, 3, 3]),\\\n",
    "        \"For percentile 30 you should only select states/actions from two first\"\n",
    "assert np.all(test_result_90[0] == [3,1]) and \\\n",
    "        np.all(test_result_90[1] == [3,3]),\\\n",
    "        \"For percentile 90 you should only select states/actions from one game\"\n",
    "assert np.all(test_result_100[0] == [3,1]) and\\\n",
    "       np.all(test_result_100[1] == [3,3]),\\\n",
    "        \"Please make sure you use >=, not >. Also double-check how you compute percentile.\"\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(elite_states, elite_actions):\n",
    "    \"\"\"\n",
    "    Given old policy and a list of elite states/actions from select_elites,\n",
    "    return new updated policy where each action probability is proportional to\n",
    "\n",
    "    policy[s_i,a_i] ~ #[occurences of si and ai in elite states/actions]\n",
    "\n",
    "    Don't forget to normalize policy to get valid probabilities and handle 0/0 case.\n",
    "    In case you never visited a state, set probabilities for all actions to 1./n_actions\n",
    "\n",
    "    :param elite_states: 1D list of states from elite sessions\n",
    "    :param elite_actions: 1D list of actions from elite sessions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    new_policy = np.zeros([n_states, n_actions])\n",
    "\n",
    "    actions_states = {}\n",
    "    for action, state in zip(elite_actions, elite_states):\n",
    "        if state not in actions_states:\n",
    "            actions_states[state] = [0] * n_actions\n",
    "        actions_states[state][action] += 1\n",
    "\n",
    "    for i in range(n_states):\n",
    "        if i in actions_states:\n",
    "            new_policy[i] = [p / sum(actions_states[i]) for p in actions_states[i]]\n",
    "        else:\n",
    "            new_policy[i] = [1 / n_actions for _ in range(n_actions)]\n",
    "    # Don't forget to set 1/n_actions for all actions in unvisited states.\n",
    "\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [1, 0, 0, 1, 0, 0], 2: [0, 1, 2, 0, 0, 0], 3: [0, 0, 0, 1, 1, 0], 4: [0, 0, 0, 1, 0, 0], 0: [1, 0, 0, 0, 0, 0]}\n",
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "elite_states, elite_actions = ([1, 2, 3, 4, 2, 0, 2, 3, 1], [0, 2, 4, 3, 2, 0, 1, 3, 3])\n",
    "\n",
    "\n",
    "new_policy = update_policy(elite_states,elite_actions)\n",
    "\n",
    "assert np.isfinite(new_policy).all(), \"Your new policy contains NaNs or +-inf. Make sure you don't divide by zero.\"\n",
    "assert np.all(new_policy>=0), \"Your new policy can't have negative action probabilities\"\n",
    "assert np.allclose(new_policy.sum(axis=-1),1), \"Your new policy should be a valid probability distribution over actions\"\n",
    "reference_answer = np.array([\n",
    "       [ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
    "       [ 0.5       ,  0.        ,  0.        ,  0.5       ,  0.        ],\n",
    "       [ 0.        ,  0.33333333,  0.66666667,  0.        ,  0.        ],\n",
    "       [ 0.        ,  0.        ,  0.        ,  0.5       ,  0.5       ]])\n",
    "assert np.allclose(new_policy[:4,:5],reference_answer)\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def show_progress(batch_rewards, log, percentile, reward_range=[-990,+10]):\n",
    "    \"\"\"\n",
    "    A convenience function that displays training progress. \n",
    "    No cool math here, just charts.\n",
    "    \"\"\"\n",
    "    \n",
    "    mean_reward, threshold = np.mean(batch_rewards), np.percentile(batch_rewards, percentile)\n",
    "    log.append([mean_reward,threshold])\n",
    "\n",
    "    clear_output(True)\n",
    "    print(\"mean reward = %.3f, threshold=%.3f\"%(mean_reward, threshold))\n",
    "    plt.figure(figsize=[8,4])\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
    "    plt.plot(list(zip(*log))[1], label='Reward thresholds')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist(batch_rewards,range=reward_range);\n",
    "    plt.vlines([np.percentile(batch_rewards, percentile)], [0], [100], label=\"percentile\", color='red')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reset policy just in case\n",
    "policy = np.ones([n_states, n_actions]) / n_actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward = -18.640, threshold=5.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-95028861e657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#display results on chart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mshow_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercentile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-160-5de728034713>\u001b[0m in \u001b[0;36mshow_progress\u001b[0;34m(batch_rewards, log, percentile, reward_range)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \"\"\"\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfigure_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mshow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-9>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2047\u001b[0m                         \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2048\u001b[0m                         \u001b[0mdryrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2049\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   2050\u001b[0m                     \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cachedRenderer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2051\u001b[0m                     \u001b[0mbbox_artists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bbox_extra_artists\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             _png.write_png(renderer._renderer, fh,\n\u001b[0;32m--> 523\u001b[0;31m                             self.figure.dpi, metadata=metadata)\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "n_sessions = 250  #sample this many sessions\n",
    "percentile = 50  #take this percent of session with highest rewards\n",
    "learning_rate = 0.5  #add this thing to all counts for stability\n",
    "\n",
    "log = []\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    %time sessions = [generate_session(policy) for _ in range(n_sessions)]\n",
    "    \n",
    "    batch_states,batch_actions,batch_rewards = zip(*sessions)\n",
    "\n",
    "    elite_states, elite_actions = select_elites(batch_states, batch_actions, batch_rewards, percentile)\n",
    "    \n",
    "    new_policy = update_policy(elite_states,elite_actions)\n",
    "    \n",
    "    policy = learning_rate * new_policy + (1-learning_rate) * policy\n",
    "    \n",
    "    #display results on chart\n",
    "    show_progress(batch_rewards, log, percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflecting on results\n",
    "\n",
    "You may have noticed that the taxi problem quickly converges from <-1000 to a near-optimal score and then descends back into -50/-100. This is in part because the environment has some innate randomness. Namely, the starting points of passenger/driver change from episode to episode.\n",
    "\n",
    "In case CEM failed to learn how to win from one distinct starting point, it will simply discard it because no sessions from that starting point will make it into the \"elites\".\n",
    "\n",
    "To mitigate that problem, you can either reduce the threshold for elite sessions (duct tape way) or  change the way you evaluate strategy (theoretically correct way). You can first sample an action for every possible state and then evaluate this choice of actions by running _several_ games and averaging rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to coursera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "from submit import submit_taxi\n",
    "submit_taxi(generate_session, policy, \"felix.boesing@t-online.de\", \"OHPkWpFOfReSooG9\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
